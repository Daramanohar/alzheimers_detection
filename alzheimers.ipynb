{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1w_WBFT4vuqG9GujqSk7Z7LICB77ixP71",
      "authorship_tag": "ABX9TyOIRbNIjh7+jFpHJFzgoVhu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daramanohar/alzheimers_detection/blob/main/alzheimers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies:"
      ],
      "metadata": {
        "id": "Re8i5-WDDSl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/alzheimers calssification.zip\" -d /content/alzheimers_classification\n",
        "\n"
      ],
      "metadata": {
        "id": "urCmXc5V-gS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define necessary directories\n",
        "directories = [\n",
        "    \"/content/alzheimers_classification/alzheimers calssification/Alzheimer_Detection/data/train\",\n",
        "    \"/content/alzheimers_classification/alzheimers calssification/Alzheimer_Detection/data/val\",\n",
        "    \"/content/models\",\n",
        "    \"/content/models/VGG16\",\n",
        "    \"/content/models/VGG19\",\n",
        "    \"/content/models/InceptionV3\"\n",
        "]\n",
        "\n",
        "# Create directories if they do not exist\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(\"All required directories are created successfully!\")\n"
      ],
      "metadata": {
        "id": "0KiIwHLafOou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas matplotlib seaborn opencv-python scikit-learn tensorflow keras tqdm albumentations pillow\n",
        "!pip install reportlab seaborn\n"
      ],
      "metadata": {
        "id": "eZLFx72UiRdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_preprocessing"
      ],
      "metadata": {
        "id": "iiYRttMQjKQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, save_img\n",
        "\n",
        "# ----- Step 1: Define Preprocessing Functions -----\n",
        "\n",
        "# 1. Segmentation: (A simplistic example using Otsu's threshold)\n",
        "def segment_brain(image):\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    # Apply Otsu's thresholding to create a mask\n",
        "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    # Use the mask to segment the image (assumes brain tissue appears brighter)\n",
        "    segmented = cv2.bitwise_and(image, image, mask=thresh)\n",
        "    return segmented\n",
        "\n",
        "# 2. Skull Stripping: (Placeholder function; for real cases, consider specialized neuroimaging tools)\n",
        "def skull_strip(image):\n",
        "    # For demonstration, assume segmentation did most of the work.\n",
        "    # You could apply further morphological operations here.\n",
        "    return image\n",
        "\n",
        "# 3. Spatial Normalization & Re-scaling: (Resize image to a standard target size)\n",
        "def spatial_normalize(image, target_size=(224, 224)):\n",
        "    normalized = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
        "    return normalized\n",
        "\n",
        "# 4. Data Augmentation: (Using Albumentations)\n",
        "augmentation_pipeline = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.5)\n",
        "])\n",
        "\n",
        "# 5. Smoothing: (Apply Gaussian Blur)\n",
        "def smooth_image(image):\n",
        "    smoothed = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "    return smoothed\n",
        "\n",
        "# ----- Step 2: Full Preprocessing Pipeline -----\n",
        "\n",
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    # Load image using Keras (PIL-based) and convert to a NumPy array\n",
        "    img = load_img(image_path)\n",
        "    image = img_to_array(img).astype(np.uint8)\n",
        "\n",
        "    # Apply segmentation to isolate the brain region\n",
        "    segmented = segment_brain(image)\n",
        "\n",
        "    # Apply skull stripping (placeholder function)\n",
        "    stripped = skull_strip(segmented)\n",
        "\n",
        "    # Spatially normalize & re-scale the image to the target size\n",
        "    normalized = spatial_normalize(stripped, target_size=target_size)\n",
        "\n",
        "    # Data augmentation: apply random flips/rotations/scaling\n",
        "    augmented = augmentation_pipeline(image=normalized)['image']\n",
        "\n",
        "    # Smoothing: reduce noise with Gaussian blur\n",
        "    smoothed = smooth_image(augmented)\n",
        "\n",
        "    # Final normalization: scale pixel values to [0, 1]\n",
        "    processed = smoothed / 255.0\n",
        "    return processed\n",
        "\n",
        "# ----- Step 3: Process Entire Folder Structure -----\n",
        "\n",
        "# Define base input and output directories (update paths as needed)\n",
        "input_base = r'/content/alzheimers_classification/alzheimers calssification/Alzheimer_Detection/data/raw/Axial'\n",
        "output_base = r'/content/alzheimers_classification/alzheimers calssification/Alzheimer_Detection/data/processed'\n",
        "categories = ['AD', 'CMI', 'CN']\n",
        "\n",
        "for category in categories:\n",
        "    input_dir = os.path.join(input_base, category)\n",
        "    output_dir = os.path.join(output_base, category)\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Process each image in the category folder\n",
        "    for img_name in os.listdir(input_dir):\n",
        "        img_path = os.path.join(input_dir, img_name)\n",
        "        try:\n",
        "            processed_img = preprocess_image(img_path, target_size=(224, 224))\n",
        "            # Convert the processed image (which is normalized) back to uint8 for saving\n",
        "            processed_img_uint8 = (processed_img * 255).astype(np.uint8)\n",
        "            output_path = os.path.join(output_dir, img_name)\n",
        "            save_img(output_path, processed_img_uint8)\n",
        "            print(f\"Processed and saved: {img_path} --> {output_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "9FgDVPWfiUJz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split your preprocessed images into training and validation"
      ],
      "metadata": {
        "id": "RagXoyBZqHCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Define the source folder where your preprocessed images are stored\n",
        "source_dir = '/content/alzheimers_classification/alzheimers calssification/Alzheimer_Detection/data/processed'\n",
        "\n",
        "# Define destination base directory for train and validation sets\n",
        "dest_base = '/content/alzheimers_classification/alzheimers calssification/Alzheimer_Detection/data'\n",
        "train_dir = os.path.join(dest_base, 'train')\n",
        "val_dir = os.path.join(dest_base, 'val')\n",
        "\n",
        "# Define the split ratio (e.g., 80% training, 20% validation)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Create the destination directories if they don't exist\n",
        "for directory in [train_dir, val_dir]:\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "# Process each category (AD, MCI, NC)\n",
        "categories = ['AD', 'CMI', 'CN']\n",
        "for category in categories:\n",
        "    source_category = os.path.join(source_dir, category)\n",
        "\n",
        "    # Create category subdirectories for train and val\n",
        "    train_category = os.path.join(train_dir, category)\n",
        "    val_category = os.path.join(val_dir, category)\n",
        "    os.makedirs(train_category, exist_ok=True)\n",
        "    os.makedirs(val_category, exist_ok=True)\n",
        "\n",
        "    # Get list of images/files for this category and filter out directories\n",
        "    images = [img for img in os.listdir(source_category) if os.path.isfile(os.path.join(source_category, img))]\n",
        "    random.shuffle(images)\n",
        "\n",
        "    # Calculate split index\n",
        "    split_index = int(len(images) * split_ratio)\n",
        "    train_images = images[:split_index]\n",
        "    val_images = images[split_index:]\n",
        "\n",
        "    # Copy images to train folder\n",
        "    for img in train_images:\n",
        "        src = os.path.join(source_category, img)\n",
        "        dst = os.path.join(train_category, img)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "    # Copy images to validation folder\n",
        "    for img in val_images:\n",
        "        src = os.path.join(source_category, img)\n",
        "        dst = os.path.join(val_category, img)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "    print(f\"Category '{category}': {len(train_images)} images in train, {len(val_images)} in val.\")\n"
      ],
      "metadata": {
        "id": "ifle6Ho9qBQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Implementation Using Transfer Learning VGG-16"
      ],
      "metadata": {
        "id": "qGiSWoXko1lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16  # Switching back to VGG16 which worked better for you\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Hyperparameters - optimized for VGG16\n",
        "IMG_SIZE = (224, 224)  # Standard size for VGG16\n",
        "BATCH_SIZE = 32  # Balanced for training stability\n",
        "EPOCHS = 50  # More epochs with early stopping\n",
        "NUM_CLASSES = 3  # AD, MCI, CN\n",
        "INITIAL_LR = 1e-4  # Lower initial learning rate for more stable training\n",
        "WEIGHT_DECAY = 1e-4  # Increased L2 regularization\n",
        "\n",
        "# Data directories (update these paths to your environment)\n",
        "train_dir = '/content/alzheimers_classification/alzheimers calssification/Alzheimer_Detection/data/train'\n",
        "val_dir = '/content/alzheimers_classification/alzheimers calssification/Alzheimer_Detection/data/val'\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "model_dir = 'Alzheimer_Detection/models'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "log_dir = os.path.join(model_dir, \"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Improved data augmentation strategy - but not too aggressive\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,           # Reduced rotation for brain scans\n",
        "    zoom_range=0.15,             # Moderate zoom\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    shear_range=0.1,             # Reduced shear (medical images)\n",
        "    horizontal_flip=True,        # Flipping can be valid for brain scans\n",
        "    vertical_flip=False,         # Brain scans typically have orientation significance\n",
        "    fill_mode='nearest',         # Changed to nearest for medical images\n",
        "    brightness_range=[0.85, 1.15], # Reduced brightness variation\n",
        "    validation_split=0.2         # Using validation split for consistency\n",
        ")\n",
        "\n",
        "# Validation data with minimal processing\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "# Data generators with class weights\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "# Calculate class weights to handle potential class imbalance\n",
        "class_weights_dict = None\n",
        "try:\n",
        "    # Get class indices\n",
        "    classes = list(train_generator.class_indices.keys())\n",
        "    # Count samples per class\n",
        "    class_counts = [0] * len(classes)\n",
        "    for i in range(len(train_generator.classes)):\n",
        "        class_counts[train_generator.classes[i]] += 1\n",
        "\n",
        "    # Calculate class weights\n",
        "    total = sum(class_counts)\n",
        "    class_weights = [total / (len(class_counts) * count) for count in class_counts]\n",
        "    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "    print(\"Class weights:\", class_weights_dict)\n",
        "except Exception as e:\n",
        "    print(f\"Could not compute class weights: {e}\")\n",
        "\n",
        "# Load VGG16 with pre-trained ImageNet weights\n",
        "base_model = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_tensor=Input(shape=(*IMG_SIZE, 3))\n",
        ")\n",
        "\n",
        "# Freeze initial layers - only train the deeper layers for feature extraction\n",
        "# For VGG16, freezing first 15 layers (out of 19) works well\n",
        "for layer in base_model.layers[:15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build enhanced classification head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)  # Global pooling instead of flatten for better generalization\n",
        "\n",
        "# First dense block\n",
        "x = Dense(1024, kernel_regularizer=l2(WEIGHT_DECAY))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = tf.keras.layers.ReLU()(x)  # ReLU works better with VGG16\n",
        "x = Dropout(0.5)(x)  # Increased dropout for better regularization\n",
        "\n",
        "# Second dense block\n",
        "x = Dense(512, kernel_regularizer=l2(WEIGHT_DECAY))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = tf.keras.layers.ReLU()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Output layer\n",
        "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "# Final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Use a fixed learning rate for training\n",
        "optimizer = Adam(learning_rate=INITIAL_LR)\n",
        "\n",
        "# Compile with gradient clipping to prevent exploding gradients\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=INITIAL_LR,\n",
        "    clipnorm=1.0  # Add gradient clipping\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),  # Reduced label smoothing\n",
        "    metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Improved callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',  # Monitor accuracy instead of loss\n",
        "    patience=10,             # More patience\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    min_delta=0.005          # Significant improvement\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',  # Monitor accuracy instead of loss\n",
        "    factor=0.2,              # More aggressive reduction\n",
        "    patience=5,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    os.path.join(model_dir, 'vgg16_best.keras'),  # Using new .keras format\n",
        "    monitor='val_accuracy',  # Monitor accuracy instead of loss\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        "    save_weights_only=False\n",
        ")\n",
        "\n",
        "# Add TensorBoard for visualization\n",
        "tensorboard = TensorBoard(\n",
        "    log_dir=log_dir,\n",
        "    histogram_freq=1,\n",
        "    write_graph=True,\n",
        "    update_freq='epoch'\n",
        ")\n",
        "\n",
        "# Initial training with semi-frozen base model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(train_generator.samples / BATCH_SIZE),\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=math.ceil(val_generator.samples / BATCH_SIZE),\n",
        "    epochs=25,  # Longer initial phase\n",
        "    callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard],\n",
        "    class_weight=class_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fine-tuning phase - unfreeze more layers\n",
        "print(\"Starting fine-tuning phase...\")\n",
        "\n",
        "# Unfreeze more layers for fine-tuning\n",
        "for layer in base_model.layers[10:]:  # Unfreeze the last few layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Use a lower learning rate for fine-tuning\n",
        "ft_optimizer = Adam(learning_rate=5e-6)  # Much lower learning rate\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model.compile(\n",
        "    optimizer=ft_optimizer,\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
        "    metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# Modify callbacks for fine-tuning phase\n",
        "ft_early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=15,             # More patience for fine-tuning\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    min_delta=0.001         # Smaller improvements matter now\n",
        ")\n",
        "\n",
        "ft_reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,              # Less aggressive reduction\n",
        "    patience=7,\n",
        "    min_lr=1e-8,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "ft_checkpoint = ModelCheckpoint(\n",
        "    os.path.join(model_dir, 'vgg16_ft_best.keras'),\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        "    save_weights_only=False\n",
        ")\n",
        "\n",
        "# Continue with fine-tuning\n",
        "current_epoch = len(history.history['loss'])\n",
        "ft_history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(train_generator.samples / BATCH_SIZE),\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=math.ceil(val_generator.samples / BATCH_SIZE),\n",
        "    epochs=current_epoch + 25,  # Additional 25 epochs\n",
        "    initial_epoch=current_epoch,\n",
        "    callbacks=[ft_early_stopping, ft_reduce_lr, ft_checkpoint, tensorboard],\n",
        "    class_weight=class_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save(os.path.join(model_dir, 'vgg16_final_model.keras'))\n",
        "\n",
        "# Combine histories\n",
        "all_history = {}\n",
        "for key in history.history:\n",
        "    all_history[key] = history.history[key].copy()\n",
        "    if key in ft_history.history:\n",
        "        all_history[key].extend(ft_history.history[key])\n",
        "\n",
        "# Wrap combined history\n",
        "class HistoryWrapper:\n",
        "    def __init__(self, history_dict):\n",
        "        self.history = history_dict\n",
        "\n",
        "# Function to visualize training history\n",
        "def plot_training_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(model_dir, 'training_history.png'))\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the training process\n",
        "try:\n",
        "    combined_history = HistoryWrapper(all_history)\n",
        "    plot_training_history(combined_history)\n",
        "except Exception as e:\n",
        "    print(f\"Could not plot training history: {e}\")\n",
        "\n",
        "# Evaluate the final model\n",
        "final_results = model.evaluate(val_generator)\n",
        "print(f\"Final validation results: {dict(zip(model.metrics_names, final_results))}\")\n",
        "\n",
        "# Plot confusion matrix to better understand model performance\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Get predictions\n",
        "val_generator.reset()\n",
        "y_pred = model.predict(val_generator)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = val_generator.classes\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=list(val_generator.class_indices.keys()),\n",
        "            yticklabels=list(val_generator.class_indices.keys()))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig(os.path.join(model_dir, 'confusion_matrix.png'))\n",
        "plt.show()\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_true, y_pred_classes,\n",
        "                              target_names=list(val_generator.class_indices.keys()),\n",
        "                              output_dict=True)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes,\n",
        "                          target_names=list(val_generator.class_indices.keys())))\n",
        "\n"
      ],
      "metadata": {
        "id": "ubgUJfWko0M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG-19"
      ],
      "metadata": {
        "id": "vfCceuHXnr0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG19  # Switching back to VGG19\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Hyperparameters - optimized for VGG19\n",
        "IMG_SIZE = (224, 224)  # Standard size for VGG19\n",
        "BATCH_SIZE = 32  # Balanced for training stability\n",
        "EPOCHS = 50  # More epochs with early stopping\n",
        "NUM_CLASSES = 3  # AD, MCI, CN\n",
        "INITIAL_LR = 1e-4  # Lower initial learning rate for more stable training\n",
        "WEIGHT_DECAY = 1e-4  # Increased L2 regularization\n",
        "\n",
        "# Data directories (update these paths to your environment)\n",
        "train_dir = '/content/alzheimers_classification/alzheimers calssification/Alzheimer_Detection/data/train'\n",
        "val_dir = '/content/alzheimers_classification/alzheimers calssification/Alzheimer_Detection/data/val'\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "model_dir = 'Alzheimer_Detection/models'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "log_dir = os.path.join(model_dir, \"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Improved data augmentation strategy - but not too aggressive\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,           # Reduced rotation for brain scans\n",
        "    zoom_range=0.15,             # Moderate zoom\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    shear_range=0.1,             # Reduced shear (medical images)\n",
        "    horizontal_flip=True,        # Flipping can be valid for brain scans\n",
        "    vertical_flip=False,         # Brain scans typically have orientation significance\n",
        "    fill_mode='nearest',         # Changed to nearest for medical images\n",
        "    brightness_range=[0.85, 1.15], # Reduced brightness variation\n",
        "    validation_split=0.2         # Using validation split for consistency\n",
        ")\n",
        "\n",
        "# Validation data with minimal processing\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "# Data generators with class weights\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "# Calculate class weights to handle potential class imbalance\n",
        "class_weights_dict = None\n",
        "try:\n",
        "    # Get class indices\n",
        "    classes = list(train_generator.class_indices.keys())\n",
        "    # Count samples per class\n",
        "    class_counts = [0] * len(classes)\n",
        "    for i in range(len(train_generator.classes)):\n",
        "        class_counts[train_generator.classes[i]] += 1\n",
        "\n",
        "    # Calculate class weights\n",
        "    total = sum(class_counts)\n",
        "    class_weights = [total / (len(class_counts) * count) for count in class_counts]\n",
        "    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "    print(\"Class weights:\", class_weights_dict)\n",
        "except Exception as e:\n",
        "    print(f\"Could not compute class weights: {e}\")\n",
        "\n",
        "# Load VGG19 with pre-trained ImageNet weights\n",
        "base_model = VGG19(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_tensor=Input(shape=(*IMG_SIZE, 3))\n",
        ")\n",
        "\n",
        "# Freeze initial layers - adjusted for VGG19's structure (19 layers total)\n",
        "for layer in base_model.layers[:17]:  # Freeze through block4_pool\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build enhanced classification head (same as VGG16 version)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# First dense block\n",
        "x = Dense(1024, kernel_regularizer=l2(WEIGHT_DECAY))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = tf.keras.layers.ReLU()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Second dense block\n",
        "x = Dense(512, kernel_regularizer=l2(WEIGHT_DECAY))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = tf.keras.layers.ReLU()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Output layer\n",
        "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "# Final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Optimizer setup (identical to VGG16 version)\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=INITIAL_LR,\n",
        "    clipnorm=1.0\n",
        ")\n",
        "\n",
        "# Compile model (identical to VGG16 version)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
        "    metrics=['accuracy', tf.keras.metrics.AUC(),\n",
        "            tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Improved callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',  # Monitor accuracy instead of loss\n",
        "    patience=10,             # More patience\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    min_delta=0.005          # Significant improvement\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',  # Monitor accuracy instead of loss\n",
        "    factor=0.2,              # More aggressive reduction\n",
        "    patience=5,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    os.path.join(model_dir, 'vgg16_best.keras'),  # Using new .keras format\n",
        "    monitor='val_accuracy',  # Monitor accuracy instead of loss\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        "    save_weights_only=False\n",
        ")\n",
        "\n",
        "# Add TensorBoard for visualization\n",
        "tensorboard = TensorBoard(\n",
        "    log_dir=log_dir,\n",
        "    histogram_freq=1,\n",
        "    write_graph=True,\n",
        "    update_freq='epoch'\n",
        ")\n",
        "\n",
        "# Initial training with semi-frozen base model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(train_generator.samples / BATCH_SIZE),\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=math.ceil(val_generator.samples / BATCH_SIZE),\n",
        "    epochs=25,  # Longer initial phase\n",
        "    callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard],\n",
        "    class_weight=class_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fine-tuning phase - unfreeze more layers\n",
        "print(\"Starting fine-tuning phase...\")\n",
        "\n",
        "# Unfreeze more layers for fine-tuning\n",
        "for layer in base_model.layers[10:]:  # Unfreeze the last few layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Use a lower learning rate for fine-tuning\n",
        "ft_optimizer = Adam(learning_rate=5e-6)  # Much lower learning rate\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model.compile(\n",
        "    optimizer=ft_optimizer,\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
        "    metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# Modify callbacks for fine-tuning phase\n",
        "ft_early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=15,             # More patience for fine-tuning\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    min_delta=0.001         # Smaller improvements matter now\n",
        ")\n",
        "\n",
        "ft_reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,              # Less aggressive reduction\n",
        "    patience=7,\n",
        "    min_lr=1e-8,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "ft_checkpoint = ModelCheckpoint(\n",
        "    os.path.join(model_dir, 'vgg16_ft_best.keras'),\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        "    save_weights_only=False\n",
        ")\n",
        "\n",
        "# Continue with fine-tuning\n",
        "current_epoch = len(history.history['loss'])\n",
        "ft_history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(train_generator.samples / BATCH_SIZE),\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=math.ceil(val_generator.samples / BATCH_SIZE),\n",
        "    epochs=current_epoch + 25,  # Additional 25 epochs\n",
        "    initial_epoch=current_epoch,\n",
        "    callbacks=[ft_early_stopping, ft_reduce_lr, ft_checkpoint, tensorboard],\n",
        "    class_weight=class_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save(os.path.join(model_dir, 'vgg19_final_model.keras'))\n",
        "\n",
        "# Combine histories\n",
        "all_history = {}\n",
        "for key in history.history:\n",
        "    all_history[key] = history.history[key].copy()\n",
        "    if key in ft_history.history:\n",
        "        all_history[key].extend(ft_history.history[key])\n",
        "\n",
        "# Wrap combined history\n",
        "class HistoryWrapper:\n",
        "    def __init__(self, history_dict):\n",
        "        self.history = history_dict\n",
        "\n",
        "# Function to visualize training history\n",
        "def plot_training_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(model_dir, 'training_history.png'))\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the training process\n",
        "try:\n",
        "    combined_history = HistoryWrapper(all_history)\n",
        "    plot_training_history(combined_history)\n",
        "except Exception as e:\n",
        "    print(f\"Could not plot training history: {e}\")\n",
        "\n",
        "# Evaluate the final model\n",
        "final_results = model.evaluate(val_generator)\n",
        "print(f\"Final validation results: {dict(zip(model.metrics_names, final_results))}\")\n",
        "\n",
        "# Plot confusion matrix to better understand model performance\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Get predictions\n",
        "val_generator.reset()\n",
        "y_pred = model.predict(val_generator)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = val_generator.classes\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=list(val_generator.class_indices.keys()),\n",
        "            yticklabels=list(val_generator.class_indices.keys()))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig(os.path.join(model_dir, 'confusion_matrix.png'))\n",
        "plt.show()\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_true, y_pred_classes,\n",
        "                              target_names=list(val_generator.class_indices.keys()),\n",
        "                              output_dict=True)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes,\n",
        "                          target_names=list(val_generator.class_indices.keys())))\n",
        "\n"
      ],
      "metadata": {
        "id": "XNkgLNghec02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INCEPTION -V3\n"
      ],
      "metadata": {
        "id": "PNjmQkWOnlSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Hyperparameters - optimized for VGG19\n",
        "IMG_SIZE = (299, 299)   # Updated input size\n",
        "BATCH_SIZE = 32  # Balanced for training stability\n",
        "EPOCHS = 50  # More epochs with early stopping\n",
        "NUM_CLASSES = 3  # AD, MCI, CN\n",
        "INITIAL_LR = 1e-4  # Lower initial learning rate for more stable training\n",
        "WEIGHT_DECAY = 1e-4  # Increased L2 regularization\n",
        "\n",
        "# Data directories (update these paths to your environment)\n",
        "train_dir = '/content/alzheimers_classification/alzheimers calssification/Alzheimer_Detection/data/train'\n",
        "val_dir = '/content/alzheimers_classification/alzheimers calssification/Alzheimer_Detection/data/val'\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "model_dir = 'Alzheimer_Detection/models'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "log_dir = os.path.join(model_dir, \"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Use Inception-specific preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,  # Scales pixels between -1 and 1\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "\n",
        "# Data generators with class weights\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "# Calculate class weights to handle potential class imbalance\n",
        "class_weights_dict = None\n",
        "try:\n",
        "    # Get class indices\n",
        "    classes = list(train_generator.class_indices.keys())\n",
        "    # Count samples per class\n",
        "    class_counts = [0] * len(classes)\n",
        "    for i in range(len(train_generator.classes)):\n",
        "        class_counts[train_generator.classes[i]] += 1\n",
        "\n",
        "    # Calculate class weights\n",
        "    total = sum(class_counts)\n",
        "    class_weights = [total / (len(class_counts) * count) for count in class_counts]\n",
        "    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "    print(\"Class weights:\", class_weights_dict)\n",
        "except Exception as e:\n",
        "    print(f\"Could not compute class weights: {e}\")\n",
        "\n",
        "# Load InceptionV3 base model\n",
        "base_model = InceptionV3(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(*IMG_SIZE, 3))\n",
        "\n",
        "\n",
        "for layer in base_model.layers[:249]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[249:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Add custom head (same structure)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "# Final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Optimizer setup (identical to VGG16 version)\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=INITIAL_LR,\n",
        "    clipnorm=1.0\n",
        ")\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5, clipnorm=1.0),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Improved callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',  # Monitor accuracy instead of loss\n",
        "    patience=10,             # More patience\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    min_delta=0.005          # Significant improvement\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',  # Monitor accuracy instead of loss\n",
        "    factor=0.2,              # More aggressive reduction\n",
        "    patience=5,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    os.path.join(model_dir, 'vgg16_best.keras'),  # Using new .keras format\n",
        "    monitor='val_accuracy',  # Monitor accuracy instead of loss\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        "    save_weights_only=False\n",
        ")\n",
        "\n",
        "# Add TensorBoard for visualization\n",
        "tensorboard = TensorBoard(\n",
        "    log_dir=log_dir,\n",
        "    histogram_freq=1,\n",
        "    write_graph=True,\n",
        "    update_freq='epoch'\n",
        ")\n",
        "\n",
        "# Initial training with semi-frozen base model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(train_generator.samples / BATCH_SIZE),\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=math.ceil(val_generator.samples / BATCH_SIZE),\n",
        "    epochs=25,  # Longer initial phase\n",
        "    callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard],\n",
        "    class_weight=class_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fine-tuning phase - unfreeze more layers\n",
        "print(\"Starting fine-tuning phase...\")\n",
        "\n",
        "# Unfreeze more layers for fine-tuning\n",
        "for layer in base_model.layers[10:]:  # Unfreeze the last few layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Use a lower learning rate for fine-tuning\n",
        "ft_optimizer = Adam(learning_rate=5e-6)  # Much lower learning rate\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model.compile(\n",
        "    optimizer=ft_optimizer,\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
        "    metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# Modify callbacks for fine-tuning phase\n",
        "ft_early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=15,             # More patience for fine-tuning\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    min_delta=0.001         # Smaller improvements matter now\n",
        ")\n",
        "\n",
        "ft_reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,              # Less aggressive reduction\n",
        "    patience=7,\n",
        "    min_lr=1e-8,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "ft_checkpoint = ModelCheckpoint(\n",
        "    os.path.join(model_dir, 'vgg16_ft_best.keras'),\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        "    save_weights_only=False\n",
        ")\n",
        "\n",
        "# Continue with fine-tuning\n",
        "current_epoch = len(history.history['loss'])\n",
        "ft_history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(train_generator.samples / BATCH_SIZE),\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=math.ceil(val_generator.samples / BATCH_SIZE),\n",
        "    epochs=current_epoch + 25,  # Additional 25 epochs\n",
        "    initial_epoch=current_epoch,\n",
        "    callbacks=[ft_early_stopping, ft_reduce_lr, ft_checkpoint, tensorboard],\n",
        "    class_weight=class_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save(os.path.join(model_dir, 'vgg19_final_model.keras'))\n",
        "\n",
        "# Combine histories\n",
        "all_history = {}\n",
        "for key in history.history:\n",
        "    all_history[key] = history.history[key].copy()\n",
        "    if key in ft_history.history:\n",
        "        all_history[key].extend(ft_history.history[key])\n",
        "\n",
        "# Wrap combined history\n",
        "class HistoryWrapper:\n",
        "    def __init__(self, history_dict):\n",
        "        self.history = history_dict\n",
        "\n",
        "# Function to visualize training history\n",
        "def plot_training_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(model_dir, 'training_history.png'))\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the training process\n",
        "try:\n",
        "    combined_history = HistoryWrapper(all_history)\n",
        "    plot_training_history(combined_history)\n",
        "except Exception as e:\n",
        "    print(f\"Could not plot training history: {e}\")\n",
        "\n",
        "# Evaluate the final model\n",
        "final_results = model.evaluate(val_generator)\n",
        "print(f\"Final validation results: {dict(zip(model.metrics_names, final_results))}\")\n",
        "\n",
        "# Plot confusion matrix to better understand model performance\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Get predictions\n",
        "val_generator.reset()\n",
        "y_pred = model.predict(val_generator)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = val_generator.classes\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=list(val_generator.class_indices.keys()),\n",
        "            yticklabels=list(val_generator.class_indices.keys()))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig(os.path.join(model_dir, 'confusion_matrix.png'))\n",
        "plt.show()\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_true, y_pred_classes,\n",
        "                              target_names=list(val_generator.class_indices.keys()),\n",
        "                              output_dict=True)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes,\n",
        "                          target_names=list(val_generator.class_indices.keys())))\n",
        "\n"
      ],
      "metadata": {
        "id": "vz-Xl8hw1Nf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9OVCVcJPF_Zv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}